{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["# Fairness-Constrained Policy Optimization (FCPO)\n\n**Implementation of long-term fairness in RL**"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["!pip install torch numpy matplotlib"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env:\n",
    "    def __init__(self):\n",
    "        self.n_s, self.n_a, self.n_g, self.gamma = 5, 2, 2, 0.95\n",
    "        self.init = {0: [0.4, 0.3, 0.2, 0.1, 0.0], 1: [0.1, 0.2, 0.3, 0.3, 0.1]}\n",
    "    def reset(self, g=None):\n",
    "        self.g = np.random.randint(2) if g is None else g\n",
    "        self.s = np.random.choice(5, p=self.init[self.g])\n",
    "        return self._obs()\n",
    "    def _obs(self):\n",
    "        o = np.zeros(7)\n",
    "        o[self.s] = 1; o[5+self.g] = 1\n",
    "        return o\n",
    "    def step(self, a):\n",
    "        r = (self.s/4)*2-0.5 if a else 0\n",
    "        if a: self.s = min(self.s+1, 4) if np.random.rand()<0.7 else max(self.s-1, 0)\n",
    "        elif np.random.rand()<0.5: self.s = max(self.s-1, 0)\n",
    "        return self._obs(), r, False, self.g\n",
    "\n",
    "env = Env()\n",
    "print('Env ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pi(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(7, 64), nn.Tanh(), nn.Linear(64, 2))\n",
    "    def forward(self, x):\n",
    "        return F.softmax(self.net(x), -1)\n",
    "\n",
    "class V(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(7, 64), nn.Tanh(), nn.Linear(64, 1))\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCPO:\n",
    "    def __init__(self, env, eps=0.15):\n",
    "        self.env, self.eps, self.gam = env, eps, env.gamma\n",
    "        self.pi = Pi().to(device)\n",
    "        self.v = V().to(device)\n",
    "        self.vg = nn.ModuleList([V().to(device) for _ in range(2)])\n",
    "        self.opt_pi = torch.optim.Adam(self.pi.parameters(), 3e-3)\n",
    "        self.opt_v = torch.optim.Adam(self.v.parameters(), 3e-3)\n",
    "        self.opt_vg = torch.optim.Adam(self.vg.parameters(), 3e-3)\n",
    "        self.lam = torch.zeros(2).to(device)\n",
    "    \n",
    "    def collect(self, n=20):\n",
    "        obs, act, rew, grp = [], [], [], []\n",
    "        for _ in range(n):\n",
    "            o = self.env.reset()\n",
    "            for _ in range(100):\n",
    "                with torch.no_grad():\n",
    "                    p = self.pi(torch.tensor(o, dtype=torch.float32).to(device))\n",
    "                    a = torch.multinomial(p, 1).item()\n",
    "                o2, r, _, g = self.env.step(a)\n",
    "                obs.append(o); act.append(a); rew.append(r); grp.append(g)\n",
    "                o = o2\n",
    "        return obs, act, rew, grp\n",
    "    \n",
    "    def rets(self, rews):\n",
    "        G, gs = 0, []\n",
    "        for r in reversed(rews):\n",
    "            G = r + self.gam * G\n",
    "            gs.insert(0, G)\n",
    "        return torch.tensor(gs, dtype=torch.float32).to(device)\n",
    "    \n",
    "    def train(self):\n",
    "        obs, act, rew, grp = self.collect()\n",
    "        O = torch.tensor(obs, dtype=torch.float32).to(device)\n",
    "        A = torch.tensor(act, dtype=torch.long).to(device)\n",
    "        G = torch.tensor(grp, dtype=torch.long).to(device)\n",
    "        R = self.rets(rew)\n",
    "        \n",
    "        # V update\n",
    "        v_loss = ((self.v(O) - R)**2).mean()\n",
    "        self.opt_v.zero_grad(); v_loss.backward(); self.opt_v.step()\n",
    "        \n",
    "        # Vg update\n",
    "        for g in range(2):\n",
    "            m = G == g\n",
    "            if m.sum() > 0:\n",
    "                vg_loss = ((self.vg[g](O[m]) - R[m])**2).mean()\n",
    "                self.opt_vg.zero_grad(); vg_loss.backward(); self.opt_vg.step()\n",
    "        \n",
    "        # Compute advantages (detached)\n",
    "        with torch.no_grad():\n",
    "            adv = R - self.v(O)\n",
    "            fadj = torch.zeros_like(adv)\n",
    "            for g in range(2):\n",
    "                m = G == g\n",
    "                if m.sum() > 0:\n",
    "                    fadj[m] = (R[m] - self.vg[g](O[m])) * self.lam[g]\n",
    "            adv_f = adv - fadj\n",
    "        \n",
    "        # Policy update - use gather for proper gradient flow\n",
    "        probs = self.pi(O)\n",
    "        log_p = torch.log(probs.gather(1, A.unsqueeze(1)).squeeze() + 1e-10)\n",
    "        loss_pi = -(log_p * adv_f).mean()\n",
    "        self.opt_pi.zero_grad(); loss_pi.backward(); self.opt_pi.step()\n",
    "        \n",
    "        # Dual update\n",
    "        with torch.no_grad():\n",
    "            g_rets = [R[G==g].mean().item() if (G==g).sum()>0 else 0 for g in range(2)]\n",
    "            disp = max(g_rets) - min(g_rets)\n",
    "            for g in range(2):\n",
    "                self.lam[g] = max(0, self.lam[g] + 0.01*(g_rets[g]-np.mean(g_rets)))\n",
    "        \n",
    "        return {'d': disp, 'gr': g_rets, 'lam': self.lam.cpu().tolist()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = FCPO(env)\n",
    "h = defaultdict(list)\n",
    "\n",
    "for i in range(100):\n",
    "    m = agent.train()\n",
    "    for k,v in m.items(): h[k].append(v)\n",
    "    if i%10==0: print(f\"{i}: D={m['d']:.3f}, G={[f'{x:.2f}' for x in m['gr']]}\")\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(10, 6))\n",
    "ax[0,0].plot(h['d']); ax[0,0].axhline(agent.eps, color='r', ls='--'); ax[0,0].set_title('Disparity')\n",
    "for g in range(2): ax[0,1].plot([x[g] for x in h['gr']], label=f'G{g}')\n",
    "ax[0,1].legend(); ax[0,1].set_title('Returns')\n",
    "for g in range(2): ax[1,0].plot([x[g] for x in h['lam']], label=f'Î»{g}')\n",
    "ax[1,0].legend(); ax[1,0].set_title('Dual Vars')\n",
    "ax[1,1].plot(h['d']); ax[1,1].set_title('Disparity')\n",
    "plt.tight_layout(); plt.show()\n",
    "print(f\"Final: {h['d'][-1]:.3f} <= {agent.eps}\")"
   ]
  }
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}},
 "nbformat": 4,
 "nbformat_minor": 5
}
