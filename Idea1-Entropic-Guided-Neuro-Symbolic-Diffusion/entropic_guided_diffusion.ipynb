{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["# Entropic-Guided Neuro-Symbolic Diffusion"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["!pip install torch transformers tree-sitter tree-sitter-python"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tree_sitter import Language, Parser\n",
    "import tree_sitter_python\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryPositionalEmbeddings(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len=8192):\n",
    "        super().__init__()\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, d_model, 2).float() / d_model))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "\n",
    "    def forward(self, x, seq_len=None):\n",
    "        if seq_len is None:\n",
    "            seq_len = x.shape[1]\n",
    "        t = torch.arange(seq_len, device=x.device, dtype=self.inv_freq.dtype)\n",
    "        freqs = torch.einsum('i,j->ij', t, self.inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        return emb[None, :, None, :]\n",
    "\n",
    "def apply_rotary_pos_emb(x, freqs):\n",
    "    cos = freqs.cos()\n",
    "    sin = freqs.sin()\n",
    "    x_even = x[..., 0::2]\n",
    "    x_odd = x[..., 1::2]\n",
    "    cos_half = cos[..., :cos.shape[-1]//2]\n",
    "    sin_half = sin[..., :sin.shape[-1]//2]\n",
    "    x_even_rot = x_even * cos_half - x_odd * sin_half\n",
    "    x_odd_rot = x_odd * cos_half + x_even * sin_half\n",
    "    x_out = torch.stack([x_even_rot, x_odd_rot], dim=-1)\n",
    "    return x_out.flatten(-2)\n",
    "\n",
    "class BidirectionalSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_head, max_len):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_head\n",
    "        self.d_head = d_model // n_head\n",
    "        self.c_attn = nn.Linear(d_model, 3 * d_model)\n",
    "        self.c_proj = nn.Linear(d_model, d_model)\n",
    "        self.rope = RotaryPositionalEmbeddings(self.d_head, max_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        qkv = self.c_attn(x).split(self.d_model, dim=2)\n",
    "        q, k, v = [z.view(B, T, self.n_head, self.d_head).transpose(1, 2) for z in qkv]\n",
    "        freqs = self.rope(q, T)\n",
    "        q = apply_rotary_pos_emb(q, freqs)\n",
    "        k = apply_rotary_pos_emb(k, freqs)\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=False)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.c_proj(y)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_head, max_len):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = BidirectionalSelfAttention(d_model, n_head, max_len)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * d_model, d_model),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class DiffusionTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, n_head=4, n_layer=4, max_len=512):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(d_model, n_head, max_len) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size)\n",
    "        self.mask_token_id = vocab_size - 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.token_embedding(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SymbolicConstraint:\n",
    "    def __init__(self):\n",
    "        self.language = Language(tree_sitter_python.language())\n",
    "        self.parser = Parser(self.language)\n",
    "    \n",
    "    def verify(self, code_bytes):\n",
    "        tree = self.parser.parse(code_bytes)\n",
    "        errors = []\n",
    "        stack = [tree.root_node]\n",
    "        while stack:\n",
    "            node = stack.pop()\n",
    "            if node.type == 'ERROR' or node.is_missing:\n",
    "                errors.append(node.byte_range)\n",
    "            stack.extend(node.children)\n",
    "        return len(errors) == 0, errors\n",
    "    \n",
    "    def get_mask_from_errors(self, code_bytes, tokens, tokenizer_decode_fn):\n",
    "        is_valid, errors = self.verify(code_bytes)\n",
    "        if is_valid:\n",
    "            return None\n",
    "        token_spans = []\n",
    "        offset = 0\n",
    "        for t in tokens:\n",
    "            s = tokenizer_decode_fn([t])\n",
    "            l = len(s.encode('utf-8'))\n",
    "            token_spans.append((offset, offset + l))\n",
    "            offset += l\n",
    "        mask_indices = []\n",
    "        for start_byte, end_byte in errors:\n",
    "            for idx, (t_start, t_end) in enumerate(token_spans):\n",
    "                if not (t_end <= start_byte or t_start >= end_byte):\n",
    "                    mask_indices.append(idx)\n",
    "        return list(set(mask_indices))\n",
    "\n",
    "class EntropicSampler:\n",
    "    def __init__(self, model, tokenizer, constraint):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.constraint = constraint\n",
    "        self.mask_id = model.mask_token_id\n",
    "    \n",
    "    def generate(self, prompt_ids, T_steps=20, output_len=20):\n",
    "        curr_seq = torch.tensor(prompt_ids + [self.mask_id] * output_len, device=device).unsqueeze(0)\n",
    "        prompt_len = len(prompt_ids)\n",
    "        print(f'Starting generation...')\n",
    "        for t in range(T_steps, 0, -1):\n",
    "            with torch.no_grad():\n",
    "                logits = self.model(curr_seq)\n",
    "            x_pred = torch.argmax(logits, dim=-1)\n",
    "            x_pred[:, :prompt_len] = torch.tensor(prompt_ids, device=device)\n",
    "            pred_tokens = x_pred[0].tolist()\n",
    "            pred_str = self.tokenizer.decode(pred_tokens)\n",
    "            error_indices = self.constraint.get_mask_from_errors(\n",
    "                pred_str.encode('utf-8'), pred_tokens, lambda t: self.tokenizer.decode(t)\n",
    "            )\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            entropy = -(probs * (probs + 1e-10).log()).sum(dim=-1)\n",
    "            if error_indices:\n",
    "                for idx in error_indices:\n",
    "                    if idx >= prompt_len:\n",
    "                        entropy[0, idx] = float('inf')\n",
    "            num_to_mask = int(output_len * t / T_steps)\n",
    "            if num_to_mask > 0:\n",
    "                _, top_k = torch.topk(entropy[0, prompt_len:], k=num_to_mask)\n",
    "                x_t_next = x_pred.clone()\n",
    "                x_t_next[0, top_k + prompt_len] = self.mask_id\n",
    "                curr_seq = x_t_next\n",
    "            else:\n",
    "                curr_seq = x_pred\n",
    "            if t % 5 == 0:\n",
    "                print(f'Step {t}: {pred_str[:40]}...')\n",
    "        return curr_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharTokenizer:\n",
    "    def __init__(self):\n",
    "        chars = sorted(set('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789_ ()[]{},.:;=\\n+-*/<>'))\n",
    "        self.vocab = {c: i for i, c in enumerate(chars)}\n",
    "        self.vocab['<MASK>'] = len(chars)\n",
    "        self.id_to_char = {i: c for c, i in self.vocab.items()}\n",
    "        self.vocab_size = len(self.vocab)\n",
    "    def encode(self, s):\n",
    "        return [self.vocab.get(c, 0) for c in s]\n",
    "    def decode(self, ids):\n",
    "        return ''.join([self.id_to_char.get(i, '?') for i in ids])\n",
    "\n",
    "tokenizer = CharTokenizer()\n",
    "model = DiffusionTransformer(vocab_size=tokenizer.vocab_size, d_model=128, n_head=4, n_layer=4).to(device)\n",
    "constraint = SymbolicConstraint()\n",
    "sampler = EntropicSampler(model, tokenizer, constraint)\n",
    "print(f'Model: {sum(p.numel() for p in model.parameters())/1e6:.2f}M params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'def add(a, b):\\n    return '\n",
    "prompt_ids = tokenizer.encode(prompt)\n",
    "output = sampler.generate(prompt_ids, T_steps=20, output_len=20)\n",
    "print(f'\\nOutput:\\n{tokenizer.decode(output[0].tolist())}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
